{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMNtYXMa9DxjWQClcl68uCa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Biruk-gebru/Diffusion-Model/blob/main/DiffusionModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZMD2fY2_Vpk7",
        "outputId": "d3cbc782-583e-411a-8025-87615ed48352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "# Install if not already present\n",
        "!pip install torch torchvision matplotlib tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform to [0, 1] and flatten to tensors\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Converts to [0,1]\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "\n",
        "# Visual check\n",
        "examples = next(iter(train_loader))[0][:8]\n",
        "fig, axs = plt.subplots(1, 8, figsize=(12, 2))\n",
        "for i in range(8):\n",
        "    axs[i].imshow(examples[i][0], cmap='gray')\n",
        "    axs[i].axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "BJ9CdAFHVvoS",
        "outputId": "7aa33e60-5baa-4a3c-c595-4c39f15d358e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.0MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 482kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.47MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.98MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x200 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SCHEDULER_LINEAR\n",
        "\n",
        "def linear_beta_schedule(timesteps, beta_start=1e-4, beta_end=0.02):\n",
        "    return torch.linspace(beta_start, beta_end, timesteps)\n",
        "\n",
        "T = 300  # Total timesteps\n",
        "betas_linear = linear_beta_schedule(T)\n",
        "alphas_linear = 1. - betas_linear\n",
        "alpha_hats_linear = torch.cumprod(alphas_linear, dim=0).to(device)\n"
      ],
      "metadata": {
        "id": "8q0NGO0OWWgT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SCHEDULER_COSINE\n",
        "\n",
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0.0001, 0.9999)\n",
        "\n",
        "betas_cosine = cosine_beta_schedule(T)\n",
        "alphas_cosine = 1. - betas_cosine\n",
        "alpha_hats_cosine = torch.cumprod(alphas_cosine, dim=0).to(device)\n"
      ],
      "metadata": {
        "id": "HaLqpx8qWdV4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SCHEDULER_EXPONENTIAL\n",
        "\n",
        "def exponential_beta_schedule(timesteps, base=1.02, beta_start=1e-4):\n",
        "    return beta_start * (base ** torch.arange(timesteps))\n",
        "\n",
        "betas_exp = exponential_beta_schedule(T)\n",
        "alphas_exp = 1. - betas_exp\n",
        "alpha_hats_exp = torch.cumprod(alphas_exp, dim=0).to(device)\n"
      ],
      "metadata": {
        "id": "TvW31Zq3WhwH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SCHEDULER_SELECTOR\n",
        "\n",
        "def get_schedule(name):\n",
        "    if name == \"linear\":\n",
        "        return betas_linear.to(device), alphas_linear.to(device), alpha_hats_linear.to(device)\n",
        "    elif name == \"cosine\":\n",
        "        return betas_cosine.to(device), alphas_cosine.to(device), alpha_hats_cosine.to(device)\n",
        "    elif name == \"exponential\":\n",
        "        return betas_exp.to(device), alphas_exp.to(device), alpha_hats_exp.to(device)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown scheduler type.\")\n",
        "\n",
        "SCHEDULER_TYPE = \"linear\"  # choose: \"linear\", \"cosine\", \"exponential\"\n",
        "betas, alphas, alpha_hats = get_schedule(SCHEDULER_TYPE)\n"
      ],
      "metadata": {
        "id": "-OxgS-LTbsyT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TIME_EMBEDDING\n",
        "\n",
        "class SinusoidalTimeEmbedding(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        self.linear1 = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.act = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, t):\n",
        "        device = t.device\n",
        "        half_dim = self.embedding_dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = t[:, None] * emb[None, :]  # Shape: [batch_size, half_dim]\n",
        "        emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=-1)  # [batch_size, embedding_dim]\n",
        "        emb = self.linear1(emb)\n",
        "        emb = self.act(emb)\n",
        "        emb = self.linear2(emb)\n",
        "        return emb  # Shape: [batch_size, embedding_dim]\n"
      ],
      "metadata": {
        "id": "Vq_xjmWmcsGi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LABEL_EMBEDDING\n",
        "\n",
        "class LabelEmbedding(nn.Module):\n",
        "    def __init__(self, num_classes=10, embed_dim=28 * 28):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(num_classes, embed_dim)\n",
        "\n",
        "    def forward(self, labels):\n",
        "        out = self.embed(labels)  # [batch_size, 784]\n",
        "        return out.view(-1, 1, 28, 28)  # Reshape to image shape\n"
      ],
      "metadata": {
        "id": "y_1WFrYFc6qb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#UNET_BACKBONE\n",
        "\n",
        "class ConditionalUNet(nn.Module):\n",
        "    def __init__(self, time_embed_dim=128):\n",
        "        super().__init__()\n",
        "        self.time_mlp = SinusoidalTimeEmbedding(time_embed_dim)\n",
        "\n",
        "        # Project time embedding to feature map shape\n",
        "        self.time_proj = nn.Sequential(\n",
        "            nn.Linear(time_embed_dim, 28 * 28),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)  # 1 (image) + 1 (label) + 1 (time)\n",
        "        self.norm1 = nn.GroupNorm(8, 64)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "        self.norm2 = nn.GroupNorm(8, 64)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 1, 3, padding=1)\n",
        "\n",
        "    def forward(self, x, t, label_embed):\n",
        "        time_embed = self.time_mlp(t)  # [batch, time_embed_dim]\n",
        "        time_embed = self.time_proj(time_embed).view(-1, 1, 28, 28)\n",
        "\n",
        "        x = torch.cat([x, time_embed, label_embed], dim=1)  # [B, 3, 28, 28]\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        return self.conv3(x)\n"
      ],
      "metadata": {
        "id": "ES40tbcjnlII"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FORWARD_DIFFUSION\n",
        "\n",
        "def add_noise(x0, t, alpha_hats, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x0)\n",
        "\n",
        "    # Ensure alpha_hats is on same device\n",
        "    sqrt_alpha_hat = alpha_hats[t].view(-1, 1, 1, 1).sqrt()\n",
        "    sqrt_one_minus_alpha_hat = (1 - alpha_hats[t]).view(-1, 1, 1, 1).sqrt()\n",
        "\n",
        "    return sqrt_alpha_hat * x0 + sqrt_one_minus_alpha_hat * noise, noise\n"
      ],
      "metadata": {
        "id": "y5lJdyuveJcw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#REVERSE_DENOISING\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, label, alpha_hats, alphas, betas, steps=T):\n",
        "    model.eval()\n",
        "    img = torch.randn((1, 1, 28, 28), device=device)\n",
        "    label_tensor = torch.tensor([label], device=device)\n",
        "    label_embed = label_embedding(label_tensor)\n",
        "\n",
        "    for t in reversed(range(steps)):\n",
        "        t_tensor = torch.tensor([t], device=device)\n",
        "        pred_noise = model(img, t_tensor, label_embed)\n",
        "\n",
        "        alpha = alphas[t]\n",
        "        alpha_hat = alpha_hats[t]\n",
        "        beta = betas[t]\n",
        "\n",
        "        # Remove predicted noise\n",
        "        img = (1 / alpha.sqrt()) * (img - ((1 - alpha).sqrt()) * pred_noise)\n",
        "\n",
        "        # Add noise for next step (except t = 0)\n",
        "        if t > 0:\n",
        "            noise = torch.randn_like(img)\n",
        "            img += beta.sqrt() * noise\n",
        "\n",
        "    return img\n"
      ],
      "metadata": {
        "id": "i1wZaOXcoOIK"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F0GmIjSjoWLo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}