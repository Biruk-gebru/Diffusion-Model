{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnC/O8wzpuFFERHH3nM/ux",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Biruk-gebru/Diffusion-Model/blob/main/DiffusionModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZMD2fY2_Vpk7",
        "outputId": "e40f49e8-9129-4d50-8f7e-65b933d160e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            " Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision matplotlib tqdm numpy\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "# Check GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\" Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NoiseScheduler:\n",
        "    \"\"\"Handles different noise scheduling strategies for diffusion models.\"\"\"\n",
        "\n",
        "    def __init__(self, timesteps: int = 1000, scheduler_type: str = \"linear\"):\n",
        "        self.timesteps = timesteps\n",
        "        self.scheduler_type = scheduler_type\n",
        "        self.betas = torch.zeros(timesteps)\n",
        "        self.alphas = torch.zeros(timesteps)\n",
        "        self.alpha_hats = torch.zeros(timesteps)\n",
        "        self._setup_schedule()\n",
        "\n",
        "    def _setup_schedule(self):\n",
        "        \"\"\"Initialize the noise schedule based on the chosen type.\"\"\"\n",
        "        if self.scheduler_type == \"linear\":\n",
        "            self.betas = self._linear_beta_schedule()\n",
        "        elif self.scheduler_type == \"cosine\":\n",
        "            self.betas = self._cosine_beta_schedule()\n",
        "        elif self.scheduler_type == \"exponential\":\n",
        "            self.betas = self._exponential_beta_schedule()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown scheduler type: {self.scheduler_type}\")\n",
        "\n",
        "        self.alphas = 1.0 - self.betas\n",
        "        self.alpha_hats = torch.cumprod(self.alphas, dim=0)\n",
        "\n",
        "    def _linear_beta_schedule(self, beta_start: float = 1e-4, beta_end: float = 0.02):\n",
        "        \"\"\"Linear noise schedule.\"\"\"\n",
        "        return torch.linspace(beta_start, beta_end, self.timesteps)\n",
        "\n",
        "    def _cosine_beta_schedule(self, s: float = 0.008):\n",
        "        \"\"\"Cosine noise schedule for better quality.\"\"\"\n",
        "        steps = self.timesteps + 1\n",
        "        x = torch.linspace(0, self.timesteps, steps)\n",
        "        alphas_cumprod = torch.cos(((x / self.timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
        "        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "        return torch.clip(betas, 0.0001, 0.9999)\n",
        "\n",
        "    def _exponential_beta_schedule(self, base: float = 1.02, beta_start: float = 1e-4):\n",
        "        \"\"\"Exponential noise schedule.\"\"\"\n",
        "        return beta_start * (base ** torch.arange(self.timesteps))\n",
        "\n",
        "    def add_noise(self, x0: torch.Tensor, t: torch.Tensor, noise: Optional[torch.Tensor] = None):\n",
        "        \"\"\"Add noise to images at timestep t.\"\"\"\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x0)\n",
        "\n",
        "        # Move scheduler tensors to the same device as input\n",
        "        device = x0.device\n",
        "        alpha_hats = self.alpha_hats.to(device)\n",
        "\n",
        "        sqrt_alpha_hat = alpha_hats[t].view(-1, 1, 1, 1).sqrt()\n",
        "        sqrt_one_minus_alpha_hat = (1 - alpha_hats[t]).view(-1, 1, 1, 1).sqrt()\n",
        "\n",
        "        return sqrt_alpha_hat * x0 + sqrt_one_minus_alpha_hat * noise, noise"
      ],
      "metadata": {
        "id": "BJ9CdAFHVvoS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalTimeEmbedding(nn.Module):\n",
        "    \"\"\"Sinusoidal time embedding for diffusion models.\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim: int):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        self.linear1 = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.act = nn.SiLU()\n",
        "        self.linear2 = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
        "        device = t.device\n",
        "        half_dim = self.embedding_dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = t[:, None] * emb[None, :]\n",
        "        emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=-1)\n",
        "\n",
        "        emb = self.linear1(emb)\n",
        "        emb = self.act(emb)\n",
        "        emb = self.linear2(emb)\n",
        "        return emb\n",
        "\n",
        "class LabelEmbedding(nn.Module):\n",
        "    \"\"\"Label embedding for conditional generation.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes: int = 10, embed_dim: int = 128):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(num_classes, embed_dim)\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(embed_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, labels: torch.Tensor) -> torch.Tensor:\n",
        "        emb = self.embed(labels)\n",
        "        return self.projection(emb)"
      ],
      "metadata": {
        "id": "weSqEGbcL7qt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionBlock(nn.Module):\n",
        "    \"\"\"Self-attention block for global reasoning.\"\"\"\n",
        "\n",
        "    def __init__(self, channels: int):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.mha = nn.MultiheadAttention(channels, 8, batch_first=True)\n",
        "        self.norm = nn.GroupNorm(8, channels)\n",
        "        self.proj = nn.Conv2d(channels, channels, 1)\n",
        "        self.gamma = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, C, H, W = x.shape\n",
        "        x_norm = self.norm(x)\n",
        "\n",
        "        x_flat = x_norm.view(B, C, H * W).permute(0, 2, 1)\n",
        "        attn_out, _ = self.mha(x_flat, x_flat, x_flat)\n",
        "        attn_out = attn_out.permute(0, 2, 1).view(B, C, H, W)\n",
        "\n",
        "        out = self.proj(attn_out)\n",
        "        return self.gamma * out + x\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    \"\"\"Residual block with time and label conditioning.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, time_embed_dim: int, label_embed_dim: int):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        # Main conv layers\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "\n",
        "        # Normalization\n",
        "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
        "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
        "\n",
        "        # Time conditioning\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_embed_dim, out_channels)\n",
        "        )\n",
        "\n",
        "        # Label conditioning\n",
        "        self.label_mlp = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(label_embed_dim, out_channels)\n",
        "        )\n",
        "\n",
        "        # Shortcut connection - handle channel mismatch\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels, out_channels, 1)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self, x: torch.Tensor, time_embed: torch.Tensor, label_embed: torch.Tensor) -> torch.Tensor:\n",
        "        h = x\n",
        "\n",
        "        # First conv\n",
        "        h = self.conv1(h)\n",
        "        h = self.norm1(h)\n",
        "\n",
        "        # Add time and label conditioning\n",
        "        time_cond = self.time_mlp(time_embed)[:, :, None, None]\n",
        "        label_cond = self.label_mlp(label_embed)[:, :, None, None]\n",
        "        h = h + time_cond + label_cond\n",
        "\n",
        "        h = F.silu(h)\n",
        "\n",
        "        # Second conv\n",
        "        h = self.conv2(h)\n",
        "        h = self.norm2(h)\n",
        "\n",
        "        # Add time and label conditioning again\n",
        "        h = h + time_cond + label_cond\n",
        "\n",
        "        h = F.silu(h)\n",
        "\n",
        "        # Add shortcut connection\n",
        "        return h + self.shortcut(x)\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "    \"\"\"Downsampling block with attention.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, time_embed_dim: int, label_embed_dim: int, use_attention: bool = True):\n",
        "        super().__init__()\n",
        "        self.res1 = ResBlock(in_channels, out_channels, time_embed_dim, label_embed_dim)\n",
        "        self.res2 = ResBlock(out_channels, out_channels, time_embed_dim, label_embed_dim)\n",
        "        self.attention = SelfAttentionBlock(out_channels) if use_attention else nn.Identity()\n",
        "        self.downsample = nn.Conv2d(out_channels, out_channels, 3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, time_embed: torch.Tensor, label_embed: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        h = self.res1(x, time_embed, label_embed)\n",
        "        h = self.res2(h, time_embed, label_embed)\n",
        "        h = self.attention(h)\n",
        "        skip = h\n",
        "        h = self.downsample(h)\n",
        "        return h, skip\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    \"\"\"Upsampling block with attention.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, time_embed_dim: int, label_embed_dim: int, use_attention: bool = True):\n",
        "        super().__init__()\n",
        "        self.upsample = nn.ConvTranspose2d(in_channels, in_channels, 4, stride=2, padding=1)\n",
        "        # Fix: Use in_channels for the concatenated input (upsampled + skip)\n",
        "        self.res1 = ResBlock(in_channels + in_channels, out_channels, time_embed_dim, label_embed_dim)\n",
        "        self.res2 = ResBlock(out_channels, out_channels, time_embed_dim, label_embed_dim)\n",
        "        self.attention = SelfAttentionBlock(out_channels) if use_attention else nn.Identity()\n",
        "\n",
        "    def forward(self, x: torch.Tensor, skip: torch.Tensor, time_embed: torch.Tensor, label_embed: torch.Tensor) -> torch.Tensor:\n",
        "        h = self.upsample(x)\n",
        "\n",
        "        # Ensure skip connection has the same spatial size\n",
        "        if h.shape[2:] != skip.shape[2:]:\n",
        "            h = F.interpolate(h, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Ensure skip connection has the same number of channels as upsampled tensor\n",
        "        if h.shape[1] != skip.shape[1]:\n",
        "            # Use 1x1 conv to match channels\n",
        "            skip = nn.Conv2d(skip.shape[1], h.shape[1], 1).to(skip.device)(skip)\n",
        "\n",
        "        h = torch.cat([h, skip], dim=1)\n",
        "        h = self.res1(h, time_embed, label_embed)\n",
        "        h = self.res2(h, time_embed, label_embed)\n",
        "        h = self.attention(h)\n",
        "        return h"
      ],
      "metadata": {
        "id": "8q0NGO0OWWgT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalUNet(nn.Module):\n",
        "    \"\"\"Proper U-Net architecture for diffusion models.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels: int = 1,\n",
        "                 model_channels: int = 128,\n",
        "                 out_channels: int = 1,\n",
        "                 num_res_blocks: int = 2,\n",
        "                 time_embed_dim: int = 128,\n",
        "                 label_embed_dim: int = 128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.model_channels = model_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.time_embed_dim = time_embed_dim\n",
        "        self.label_embed_dim = label_embed_dim\n",
        "\n",
        "        # Time embedding\n",
        "        self.time_embed = SinusoidalTimeEmbedding(time_embed_dim)\n",
        "\n",
        "        # Label embedding\n",
        "        self.label_embed = LabelEmbedding(embed_dim=label_embed_dim)\n",
        "\n",
        "        # Initial convolution\n",
        "        self.input_conv = nn.Conv2d(in_channels, model_channels, 3, padding=1)\n",
        "\n",
        "        # Downsampling path - simpler structure\n",
        "        self.down1 = DownBlock(model_channels, model_channels, time_embed_dim, label_embed_dim)\n",
        "        self.down2 = DownBlock(model_channels, model_channels, time_embed_dim, label_embed_dim)\n",
        "        self.down3 = DownBlock(model_channels, model_channels, time_embed_dim, label_embed_dim)\n",
        "\n",
        "        # Middle\n",
        "        self.middle = nn.ModuleList([\n",
        "            ResBlock(model_channels, model_channels, time_embed_dim, label_embed_dim),\n",
        "            SelfAttentionBlock(model_channels),\n",
        "            ResBlock(model_channels, model_channels, time_embed_dim, label_embed_dim)\n",
        "        ])\n",
        "\n",
        "        # Upsampling path\n",
        "        self.up3 = UpBlock(model_channels, model_channels, time_embed_dim, label_embed_dim)\n",
        "        self.up2 = UpBlock(model_channels, model_channels, time_embed_dim, label_embed_dim)\n",
        "        self.up1 = UpBlock(model_channels, model_channels, time_embed_dim, label_embed_dim)\n",
        "\n",
        "        # Output\n",
        "        self.output = nn.Sequential(\n",
        "            nn.GroupNorm(8, model_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(model_channels, out_channels, 3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
        "        # Time and label embeddings\n",
        "        time_embed = self.time_embed(t)\n",
        "        label_embed = self.label_embed(labels)\n",
        "\n",
        "        # Initial conv\n",
        "        h = self.input_conv(x)\n",
        "\n",
        "        # Downsampling with skip connections\n",
        "        h1, skip1 = self.down1(h, time_embed, label_embed)\n",
        "        h2, skip2 = self.down2(h1, time_embed, label_embed)\n",
        "        h3, skip3 = self.down3(h2, time_embed, label_embed)\n",
        "\n",
        "        # Middle\n",
        "        for block in self.middle:\n",
        "            if isinstance(block, SelfAttentionBlock):\n",
        "                h3 = block(h3)\n",
        "            else:\n",
        "                h3 = block(h3, time_embed, label_embed)\n",
        "\n",
        "        # Upsampling with skip connections\n",
        "        h = self.up3(h3, skip3, time_embed, label_embed)\n",
        "        h = self.up2(h, skip2, time_embed, label_embed)\n",
        "        h = self.up1(h, skip1, time_embed, label_embed)\n",
        "\n",
        "        # Output\n",
        "        return self.output(h)"
      ],
      "metadata": {
        "id": "HaLqpx8qWdV4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionSampler:\n",
        "    \"\"\"Handles the reverse diffusion sampling process.\"\"\"\n",
        "\n",
        "    def __init__(self, scheduler: NoiseScheduler):\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self,\n",
        "               model: nn.Module,\n",
        "               labels: torch.Tensor,\n",
        "               batch_size: int = 1,\n",
        "               image_size: int = 28,\n",
        "               channels: int = 1,\n",
        "               steps: Optional[int] = None) -> torch.Tensor:\n",
        "        \"\"\"Sample images using the trained diffusion model.\"\"\"\n",
        "        model.eval()\n",
        "\n",
        "        if steps is None:\n",
        "            steps = self.scheduler.timesteps\n",
        "\n",
        "        # Start from pure noise\n",
        "        device = next(model.parameters()).device\n",
        "        x = torch.randn(batch_size, channels, image_size, image_size, device=device)\n",
        "\n",
        "        # Move scheduler tensors to device\n",
        "        alphas = self.scheduler.alphas.to(device)\n",
        "        alpha_hats = self.scheduler.alpha_hats.to(device)\n",
        "        betas = self.scheduler.betas.to(device)\n",
        "\n",
        "        # Reverse diffusion process\n",
        "        for t in reversed(range(steps)):\n",
        "            t_tensor = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
        "\n",
        "            # Predict noise\n",
        "            predicted_noise = model(x, t_tensor, labels)\n",
        "\n",
        "            # Get current alpha and beta values\n",
        "            alpha = alphas[t]\n",
        "            alpha_hat = alpha_hats[t]\n",
        "            beta = betas[t]\n",
        "\n",
        "            # Denoising step\n",
        "            if t > 0:\n",
        "                noise = torch.randn_like(x)\n",
        "            else:\n",
        "                noise = torch.zeros_like(x)\n",
        "\n",
        "            # DDPM sampling equation\n",
        "            x = (1 / alpha.sqrt()) * (x - ((1 - alpha) / (1 - alpha_hat).sqrt()) * predicted_noise) + beta.sqrt() * noise\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "TvW31Zq3WhwH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_samples(samples: torch.Tensor, labels: torch.Tensor, num_samples: int = 8):\n",
        "    \"\"\"Visualize generated samples.\"\"\"\n",
        "    samples = samples[:num_samples].cpu()\n",
        "    labels = labels[:num_samples].cpu()\n",
        "\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        axes[i].imshow(samples[i][0], cmap='gray')\n",
        "        axes[i].set_title(f'Label: {labels[i].item()}')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def save_checkpoint(model: nn.Module, optimizer: torch.optim.Optimizer,\n",
        "                   scheduler, epoch: int, loss: float, path: str):\n",
        "    \"\"\"Save model checkpoint.\"\"\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'loss': loss,\n",
        "    }, path)\n",
        "\n",
        "def generate_from_prompt(prompt: str, model_path: str = \"best_diffusion_model.pth\", num_samples: int = 1):\n",
        "    \"\"\"Generate images from a text prompt.\"\"\"\n",
        "    # Simple prompt parsing for MNIST digits\n",
        "    digit_mapping = {\n",
        "        'zero': 0, '0': 0, 'digit 0': 0,\n",
        "        'one': 1, '1': 1, 'digit 1': 1,\n",
        "        'two': 2, '2': 2, 'digit 2': 2,\n",
        "        'three': 3, '3': 3, 'digit 3': 3,\n",
        "        'four': 4, '4': 4, 'digit 4': 4,\n",
        "        'five': 5, '5': 5, 'digit 5': 5,\n",
        "        'six': 6, '6': 6, 'digit 6': 6,\n",
        "        'seven': 7, '7': 7, 'digit 7': 7,\n",
        "        'eight': 8, '8': 8, 'digit 8': 8,\n",
        "        'nine': 9, '9': 9, 'digit 9': 9,\n",
        "    }\n",
        "\n",
        "    # Parse prompt\n",
        "    prompt_lower = prompt.lower().strip()\n",
        "    if prompt_lower in digit_mapping:\n",
        "        label = digit_mapping[prompt_lower]\n",
        "    else:\n",
        "        import random\n",
        "        label = random.randint(0, 9)\n",
        "        print(f\"Prompt '{prompt}' not recognized. Generating random digit {label}\")\n",
        "\n",
        "    # Load model\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    scheduler = NoiseScheduler(timesteps=1000, scheduler_type=\"cosine\")\n",
        "    model = ConditionalUNet(\n",
        "        in_channels=1,\n",
        "        model_channels=128,\n",
        "        out_channels=1,\n",
        "        num_res_blocks=2,\n",
        "        time_embed_dim=128,\n",
        "        label_embed_dim=128\n",
        "    )\n",
        "\n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "\n",
        "    # Generate\n",
        "    sampler = DiffusionSampler(scheduler)\n",
        "    labels = torch.full((num_samples,), label, device=device)\n",
        "    samples = sampler.sample(model, labels, batch_size=num_samples, steps=50)\n",
        "\n",
        "    return samples, label"
      ],
      "metadata": {
        "id": "-OxgS-LTbsyT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main training function optimized for Colab.\"\"\"\n",
        "    # Device setup with GPU detection\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "    # Data setup with proper Colab paths\n",
        "    print(\"Loading MNIST dataset...\")\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Use Colab-friendly data path\n",
        "    data_path = \"./data\" if not torch.cuda.is_available() else \"/content/data\"\n",
        "    train_data = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)\n",
        "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "    print(f\"Dataset loaded: {len(train_data)} training samples\")\n",
        "\n",
        "    # Model setup\n",
        "    print(\"Initializing model...\")\n",
        "    scheduler = NoiseScheduler(timesteps=1000, scheduler_type=\"cosine\")\n",
        "    model = ConditionalUNet(\n",
        "        in_channels=1,\n",
        "        model_channels=128,\n",
        "        out_channels=1,\n",
        "        num_res_blocks=2,\n",
        "        time_embed_dim=128,\n",
        "        label_embed_dim=128\n",
        "    )\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Model parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
        "\n",
        "    # Training setup\n",
        "    trainer = DiffusionTrainer(\n",
        "        model=model,\n",
        "        scheduler=scheduler,\n",
        "        device=device,\n",
        "        learning_rate=1e-4\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 20\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    print(f\"Starting training for {num_epochs} epochs...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_loss = trainer.train_epoch(train_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            save_checkpoint(\n",
        "                model, trainer.optimizer, trainer.lr_scheduler,\n",
        "                epoch, avg_loss, \"best_diffusion_model.pth\"\n",
        "            )\n",
        "            print(f\" New best model saved! Loss: {best_loss:.6f}\")\n",
        "\n",
        "        # Generate samples every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(\"Generating sample images...\")\n",
        "            sampler = DiffusionSampler(scheduler)\n",
        "            test_labels = torch.arange(10, device=device)\n",
        "            samples = sampler.sample(model, test_labels, batch_size=10, steps=50)\n",
        "            visualize_samples(samples, test_labels)\n",
        "\n",
        "\n",
        "\n",
        "# Start training\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Vq_xjmWmcsGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate images from prompts\n",
        "samples, label = generate_from_prompt(\"digit 5\")\n",
        "\n",
        "# Display\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(samples[0][0].cpu(), cmap='gray')\n",
        "plt.title(f'Generated: Digit {label}')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y_1WFrYFc6qb",
        "outputId": "00b758ed-68e8-45e0-f47b-b7a0a7e0051d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gh_s2gk7qQ8X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}